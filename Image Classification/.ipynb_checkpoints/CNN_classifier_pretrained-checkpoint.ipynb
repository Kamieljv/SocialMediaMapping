{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification Using Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Modified on Tue 18 Jun 2018\n",
    "@author: Kamieljv (GitHub)\n",
    "CNN_classifier_pretrained.ipynb:\n",
    "    loads labeled satellite imagery, structured in folders\n",
    "    runs a CNN algorithm to learn to classify the images\n",
    "    evaluates the model according to standart performance metrics\n",
    "    does a prediction of unlabeled images based on the learned model\n",
    "    \n",
    "    \n",
    "Code adapted from 'Maratyszcza' (GitHub)\n",
    "\n",
    "https://github.com/pytorch/examples/blob/42e5b996718797e45c46a25c55b031e6768f8440/imagenet/main.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import easydict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different pre-trained models to choose from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = sorted(name for name in models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "    and callable(models.__dict__[name]))\n",
    "print(model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        #target = target.cuda(async=True) #move target tensor to GPU\n",
    "        input_var = torch.autograd.Variable(input)\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "        top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                data_time=data_time, loss=losses, top1=top1, top5=top5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "        #target = target.cuda(async=True) #move target tensor to GPU\n",
    "        input_var = torch.autograd.Variable(input, volatile=True)\n",
    "        target_var = torch.autograd.Variable(target, volatile=True)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "        top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1, top5=top5))\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'\n",
    "          .format(top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, root_dir, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, root_dir + filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(root_dir + filename, root_dir + 'model_best.pth.tar')\n",
    "    print('Checkpoint Saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = args.lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measures(val_loader, model, len, batch_size):\n",
    "    model.eval()\n",
    "    conf = np.zeros((2,2))\n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "        print(\"Progress: {} %\".format(round(i/(len/batch_size)*100,2)))\n",
    "        input_var = torch.autograd.Variable(input, volatile=True)\n",
    "        output = model(input_var)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        target = target.view(1, -1)[0]\n",
    "        conf += confusion_matrix(target, predicted, [1, 0])\n",
    "    tp = conf[0,0]\n",
    "    fp = conf[0,1]\n",
    "    fn = conf[1,0]\n",
    "    tn = conf[1,1]\n",
    "    acc = (tp+tn)/(tp+tn+fp+fn)\n",
    "    prec = tp/(tp+fp)\n",
    "    rec = tp/(tp+fn)\n",
    "    return acc, prec, rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_prec1 = 0\n",
    "\n",
    "def main():\n",
    "    global args, best_prec1\n",
    "    args = easydict.EasyDict({\n",
    "        \"data\": '', #path to folder with images\n",
    "        \"arch\": 'densenet121', #chosen pre-trained model\n",
    "        \"workers\": 0,\n",
    "        \"epochs\": 10,\n",
    "        \"start_epoch\": 0,\n",
    "        \"batch_size\": 4,\n",
    "        \"lr\": 0.001,\n",
    "        \"momentum\": 0.9,\n",
    "        \"weight_decay\": 1e-4,\n",
    "        \"print_freq\": 10,\n",
    "        \"resume\": '', #path to checkpoint file\n",
    "        \"model_best\": '', #path to model_best file\n",
    "        \"evaluate\": True,\n",
    "        \"pretrained\": True,\n",
    "        \"cuda\": False, #set True if training on GPU\n",
    "        \"check_dir\": '' #path to checkpoint folder\n",
    "    })\n",
    "    \n",
    "    print(\"=> training with the following parameters:\")\n",
    "    print(args)\n",
    "\n",
    "    # create model\n",
    "    if args.pretrained:\n",
    "        print(\"=> using pre-trained model '{}'\".format(args.arch))\n",
    "        model = models.__dict__[args.arch](pretrained=True)\n",
    "    else:\n",
    "        print(\"=> creating model '{}'\".format(args.arch))\n",
    "        model = models.__dict__[args.arch]()\n",
    "    #set up GPU (if applicable) \n",
    "    if args.cuda: ##I ADDED THIS\n",
    "        if args.arch.startswith('alexnet') or args.arch.startswith('vgg'):\n",
    "            model.features = torch.nn.DataParallel(model.features)\n",
    "            model.cuda()\n",
    "        else:\n",
    "            model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    if args.model_best:\n",
    "        if os.path.isfile(args.model_best):\n",
    "            print(\"=> loading best model '{}'\".format(args.model_best))\n",
    "            best = torch.load(args.model_best)\n",
    "            args.start_epoch = best['epoch']\n",
    "            best_prec1 = best['best_prec1']\n",
    "            model.load_state_dict(best['state_dict'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(args.model_best, best['epoch']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(args.model_best))\n",
    "                  \n",
    "    elif args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
    "            checkpoint = torch.load(args.resume)\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            best_prec1 = checkpoint['best_prec1']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(args.resume, checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # Data loading code\n",
    "    traindir = os.path.join(args.data, 'train')\n",
    "    valdir = os.path.join(args.data, 'val')\n",
    "    normalize = transforms.Normalize(mean = [0.20572032, 0.28209996, 0.29277292], \n",
    "                                     std = [0.065873735, 0.08655865, 0.1461992]) #Compute mean and std with ImageStats.ipynb\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.ImageFolder(traindir, transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])),\n",
    "        batch_size=args.batch_size, shuffle=True,\n",
    "        num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "    val_data_folder = datasets.ImageFolder(valdir, transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,]))\n",
    "    val_loader = torch.utils.data.DataLoader(val_data_folder,\n",
    "        batch_size=args.batch_size, shuffle=False,\n",
    "        num_workers=args.workers, pin_memory=True)\n",
    "    \n",
    "    print(\"=> initialized data loaders and transformations\")\n",
    "    # define loss function (criterion) and pptimizer\n",
    "    if args.cuda:\n",
    "        criterion = nn.CrossEntropyLoss().cuda()\n",
    "    else: \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
    "                                momentum=args.momentum,\n",
    "                                weight_decay=args.weight_decay)\n",
    "    print(\"=> defined optimizer function\")\n",
    "    if args.evaluate:\n",
    "        val_len = len(val_data_folder)\n",
    "        acc, prec, rec = measures(val_loader, model, val_len, args.batch_size)\n",
    "        print(\"Accuracy: {}, Precision: {}, Recall: {}\".format(acc, prec, rec))\n",
    "#         validate(val_loader, model, criterion)\n",
    "        return\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "        # train for one epoch\n",
    "        train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        prec1 = validate(val_loader, model, criterion)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': args.arch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "        }, is_best, args.check_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def load_lbl_lst(filename, image_dir):\n",
    "    objects = dict()\n",
    "    with open(filename, 'rb') as f:\n",
    "        try:\n",
    "            while True:\n",
    "                objects.update(pickle.load(f))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return objects[image_dir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def save_pickle():\n",
    "    dictio = {image_dir: lbl_lst, \"reviewed_nr\":i+1}\n",
    "    today = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    filename = '' #filename for label progress file\n",
    "    pickle.dump(dictio, open(filename+today+'.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def load_grid_data(filename):\n",
    "    grid_data = []\n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for line in reader:\n",
    "            if line[0]=='id':\n",
    "                header = line\n",
    "            else:\n",
    "                grid_data.append(line)\n",
    "    return grid_data, header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the best model\n",
    "model_best = '' #path to model_best file\n",
    "arch = 'densenet121' #chosen pre-trained model\n",
    "\n",
    "print(\"=> using pre-trained model '{}'\".format(arch))\n",
    "model = models.__dict__[arch](pretrained=True)\n",
    "    \n",
    "if os.path.isfile(model_best):\n",
    "    print(\"=> loading best model '{}'\".format(model_best))\n",
    "    best = torch.load(model_best)\n",
    "    best_prec1 = best['best_prec1']\n",
    "    model.load_state_dict(best['state_dict'])\n",
    "    print(\"=> loaded checkpoint '{}' (best prec1 {})\"\n",
    "          .format(model_best, best['best_prec1']))\n",
    "else:\n",
    "    print(\"=> no checkpoint found at '{}'\".format(model_best))\n",
    "\n",
    "testdir = '' #path to images to label\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.ImageFolder(testdir, transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean = [0.20572032, 0.28209996, 0.29277292], \n",
    "                                std = [0.065873735, 0.08655865, 0.1461992]),\n",
    "        ])),\n",
    "        batch_size=1, shuffle=False,\n",
    "        num_workers=0, pin_memory=True)\n",
    "\n",
    "# Loading label list\n",
    "file = '' #path to progress file (pickle)\n",
    "lbl_lst = load_lbl_lst(file, testdir)\n",
    "total = len(lbl_lst)\n",
    "\n",
    "# Running predictions\n",
    "model.eval()\n",
    "labels = []\n",
    "for i, data in enumerate(test_loader):\n",
    "    if i%100==99:\n",
    "        print(\"Progress: {} %\".format(round(i/total*100,2)))\n",
    "        save_pickle()\n",
    "    if lbl_lst[i]==-1:\n",
    "        print('labeling...')\n",
    "        input = data[0]\n",
    "        input_var = torch.autograd.Variable(input, volatile=True)\n",
    "        output = model(input_var)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        lbl_lst[i] = predicted[0].item()\n",
    "save_pickle()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
